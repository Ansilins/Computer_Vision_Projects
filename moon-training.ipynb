{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11542422,"sourceType":"datasetVersion","datasetId":7238497}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pycocotools pandas pillow matplotlib torch torchvision tqdm ipython","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-25T12:37:30.076449Z","iopub.execute_input":"2025-04-25T12:37:30.076767Z","iopub.status.idle":"2025-04-25T12:37:33.197193Z","shell.execute_reply.started":"2025-04-25T12:37:30.076744Z","shell.execute_reply":"2025-04-25T12:37:33.196235Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.1.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.50)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.19.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (2.4.1)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pycocotools) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pycocotools) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pycocotools) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, models\nfrom pycocotools.coco import COCO\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm  # Added for progress bars","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths\ndata_folder = '/kaggle/input/moon-ans'\noutput_folder = '/kaggle/working/'\ntrain_img_dir = os.path.join(data_folder, 'train')\nvalid_img_dir = os.path.join(data_folder, 'valid')\ntest_img_dir = os.path.join(data_folder, 'test')\ntrain_json = os.path.join(data_folder, 'train', '_annotations.coco.json')\nvalid_json = os.path.join(data_folder, 'valid', '_annotations.coco.json')\ntest_json = os.path.join(data_folder, 'test', '_annotations.coco.json')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom COCO Dataset\nclass CocoDataset(torch.utils.data.Dataset):\n    def __init__(self, img_dir, coco_json, transform=None):\n        self.img_dir = img_dir\n        self.coco = COCO(coco_json)\n        self.transform = transform\n        self.img_ids = self.coco.getImgIds()\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        img_info = self.coco.loadImgs(img_id)[0]\n        img_path = os.path.join(self.img_dir, img_info['file_name'])\n        img = Image.open(img_path).convert('RGB')\n        \n        img = transforms.ToTensor()(img)\n        if self.transform:\n            img = self.transform(img)\n        \n        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n        anns = self.coco.loadAnns(ann_ids)\n        boxes = []\n        for ann in anns:\n            x, y, w, h = ann['bbox']\n            boxes.append([x, y, x + w, y + h])\n        \n        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32)\n        labels = torch.ones(boxes.size(0), dtype=torch.int64) if boxes.size(0) > 0 else torch.zeros((0,), dtype=torch.int64)\n        \n        target = {\n            'boxes': boxes,\n            'labels': labels,\n            'image_id': torch.tensor([img_id])\n        }\n        return img, target, img_id, img_info['file_name']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom Collate Function\ndef custom_collate_fn(batch):\n    imgs, targets, img_ids, file_names = zip(*batch)\n    imgs = torch.stack(imgs, dim=0)\n    return imgs, list(targets), list(img_ids), list(file_names)\n\n# mAP Calculation\ndef calculate_mAP(pred_boxes, pred_scores, true_boxes, iou_threshold=0.5):\n    if len(pred_boxes) == 0 or len(true_boxes) == 0:\n        return 0.0\n    ious = []\n    for pred in pred_boxes:\n        for true in true_boxes:\n            iou = calculate_iou(pred, true)\n            ious.append(iou)\n    ious = np.array(ious).reshape(len(pred_boxes), len(true_boxes))\n    tp = (ious >= iou_threshold).sum()\n    precision = tp / len(pred_boxes) if len(pred_boxes) > 0 else 0\n    return precision\n\ndef calculate_iou(box1, box2):\n    x1, y1, x2, y2 = box1\n    x3, y3, x4, y4 = box2\n    xi1 = max(x1, x3)\n    yi1 = max(y1, y3)\n    xi2 = min(x2, x4)\n    yi2 = min(y2, y4)\n    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n    box1_area = (x2 - x1) * (y2 - y1)\n    box2_area = (x4 - x3) * (y4 - y3)\n    union_area = box1_area + box2_area - inter_area\n    return inter_area / union_area if union_area > 0 else 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Training Function\ndef train_model(model, train_loader, valid_loader, epochs=15):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    # Updated optimizer with momentum (via betas) and weight decay as per paper\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), weight_decay=0.0005)\n    best_mAP = 0\n    best_model_path = os.path.join(output_folder, 'faster_rcnn_best.pth')\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        valid_samples = 0\n        # Add tqdm for training loop\n        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]', leave=False)\n        for imgs, targets, _, _ in train_pbar:\n            imgs = imgs.to(device)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            loss_dict = model(imgs, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            \n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n            train_loss += losses.item()\n            valid_samples += 1\n            \n            # Update progress bar with running loss\n            train_pbar.set_postfix({'loss': f'{losses.item():.4f}'})\n        \n        # Validation\n        model.eval()\n        valid_mAP = 0\n        valid_count = 0\n        # Add tqdm for validation loop\n        valid_pbar = tqdm(valid_loader, desc=f'Epoch {epoch+1}/{epochs} [Valid]', leave=False)\n        with torch.no_grad():\n            for imgs, targets, _, _ in valid_pbar:\n                imgs = imgs.to(device)\n                outputs = model(imgs)\n                for i, output in enumerate(outputs):\n                    true_boxes = targets[i]['boxes'].cpu().numpy()\n                    if len(true_boxes) == 0:\n                        continue\n                    pred_boxes = output['boxes'].cpu().numpy()\n                    pred_scores = output['scores'].cpu().numpy()\n                    mask = pred_scores > 0.5\n                    pred_boxes = pred_boxes[mask]\n                    pred_scores = pred_scores[mask]\n                    valid_mAP += calculate_mAP(pred_boxes, pred_scores, true_boxes)\n                    valid_count += 1\n        \n        train_loss_avg = train_loss / valid_samples if valid_samples > 0 else 0\n        valid_mAP_avg = valid_mAP / valid_count if valid_count > 0 else 0\n        print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss_avg:.4f}, mAP@0.5: {valid_mAP_avg:.4f}')\n        \n        if valid_mAP_avg > best_mAP:\n            best_mAP = valid_mAP_avg\n            torch.save(model.state_dict(), best_model_path)\n            print(f'Saved best model with mAP@0.5: {best_mAP:.4f}')\n    \n    torch.save(model.state_dict(), os.path.join(output_folder, 'faster_rcnn_final.pth'))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prediction Function\ndef generate_predictions(model, test_loader):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n    predictions = []\n    invalid_boxes = 0\n    \n    # Add tqdm for prediction loop\n    test_pbar = tqdm(test_loader, desc='Predicting', leave=False)\n    with torch.no_grad():\n        for imgs, targets, img_ids, file_names in test_pbar:\n            imgs = imgs.to(device)\n            outputs = model(imgs)\n            for i, (img_id, file_name, output) in enumerate(zip(img_ids, file_names, outputs)):\n                pred_boxes = output['boxes'].cpu().numpy()\n                pred_scores = output['scores'].cpu().numpy()\n                mask = pred_scores > 0.5\n                pred_boxes = pred_boxes[mask]\n                \n                for box in pred_boxes:\n                    x_min, y_min, x_max, y_max = box\n                    w = max(0, x_max - x_min)\n                    h = max(0, y_max - y_min)\n                    if w * h == 0:\n                        invalid_boxes += 1\n                        continue\n                    diameter = np.sqrt(w * h)\n                    predictions.append({\n                        'image_id': img_id,\n                        'file_name': file_name,\n                        'x': x_min,\n                        'y': y_min,\n                        'width': w,\n                        'height': h,\n                        'diameter': diameter\n                    })\n    \n    print(f\"Generated {len(predictions)} predictions. Skipped {invalid_boxes} invalid boxes (w or h <= 0).\")\n    pd.DataFrame(predictions).to_csv(os.path.join(output_folder, 'crater_predictions.csv'), index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Main\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.RandomResizedCrop(640, scale=(0.8, 1.2)),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nvalid_test_transform = transforms.Compose([\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = CocoDataset(train_img_dir, train_json, transform=train_transform)\nvalid_dataset = CocoDataset(valid_img_dir, valid_json, transform=valid_test_transform)\ntest_dataset = CocoDataset(test_img_dir, test_json, transform=valid_test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)\nvalid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)\n\n# Load Faster R-CNN with ResNet-50-FPN\nmodel = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 2  # 1 class (crater) + background\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n\ntrain_model(model, train_loader, valid_loader, epochs=15)\ngenerate_predictions(model, test_loader)\n\nprint('Training complete. Models saved as faster_rcnn_best.pth and faster_rcnn_final.pth')\nprint('Predictions saved as crater_predictions.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T12:37:33.198500Z","iopub.execute_input":"2025-04-25T12:37:33.198721Z","iopub.status.idle":"2025-04-25T13:47:50.836627Z","shell.execute_reply.started":"2025-04-25T12:37:33.198703Z","shell.execute_reply":"2025-04-25T13:47:50.835663Z"}},"outputs":[{"name":"stdout","text":"loading annotations into memory...\nDone (t=0.41s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.04s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.02s)\ncreating index...\nindex created!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15, Loss: 1.2653, mAP@0.5: 0.3295\nSaved best model with mAP@0.5: 0.3295\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/15, Loss: 1.3051, mAP@0.5: 0.6414\nSaved best model with mAP@0.5: 0.6414\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/15, Loss: 1.3102, mAP@0.5: 0.4542\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/15, Loss: 1.3102, mAP@0.5: 0.4711\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/15, Loss: 1.3153, mAP@0.5: 0.4679\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/15, Loss: 1.3115, mAP@0.5: 0.4763\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/15, Loss: 1.3014, mAP@0.5: 0.6914\nSaved best model with mAP@0.5: 0.6914\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/15, Loss: 1.3082, mAP@0.5: 0.1538\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/15, Loss: 1.3136, mAP@0.5: 0.7066\nSaved best model with mAP@0.5: 0.7066\n","output_type":"stream"},{"name":"stderr","text":"                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/15, Loss: 1.3016, mAP@0.5: 0.3660\n","output_type":"stream"},{"name":"stderr","text":"                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11/15, Loss: 1.3086, mAP@0.5: 0.7078\nSaved best model with mAP@0.5: 0.7078\n","output_type":"stream"},{"name":"stderr","text":"                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12/15, Loss: 1.3091, mAP@0.5: 0.6613\n","output_type":"stream"},{"name":"stderr","text":"                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13/15, Loss: 1.3104, mAP@0.5: 0.6691\n","output_type":"stream"},{"name":"stderr","text":"                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14/15, Loss: 1.3050, mAP@0.5: 0.6981\n","output_type":"stream"},{"name":"stderr","text":"                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 15/15, Loss: 1.3062, mAP@0.5: 0.6492\n","output_type":"stream"},{"name":"stderr","text":"                                                           ","output_type":"stream"},{"name":"stdout","text":"Generated 703 predictions. Skipped 0 invalid boxes (w or h <= 0).\nTraining complete. Models saved as faster_rcnn_best.pth and faster_rcnn_final.pth\nPredictions saved as crater_predictions.csv\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":2}]}